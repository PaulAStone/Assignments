---
title: "Week 1 Exercise: Basic R"
author: "Z620: Quantitative Biodiversity, Indiana University"
date: "January 16, 2015"
output: pdf_document
---

## OVERVIEW
This exercise will introduce some of the basic features of the R computing environment. We will briefly cover operators, data types, and simple commands that will be useful for you during the course and beyond. In addition to using R's base package, we wil also use contributed packages, which together will allow us to visualize data and peform relatively simple statistics (e.g., linear regression and ANOVA).

## HOW WE WILL BE USING R AND OTHER TOOLS
During the course, we will use [RStudio](http://www.rstudio.com/), which is a relatively user-friendly integrated developement environment that allows R to interface with other tools. For example, you are currently working in an [R Markdown](<http://rmarkdown.rstudio.com>) document. Markdown is a simple formatting syntax for authoring HTML, PDF, and other documents. 

We will also use a tool called [knitr](http://yihui.name/knitr/), which is a package that generates reports from R script and Markdown text. For example, when you click the **Knit PDF** button in the scripting window of Rstudio, a document will be generated that includes both the content as well as the output of any embedded R code. 

If there are errors in your markdown document, however, you will not be able to knit a PDF file. Assignments in this class will require that you successfully create a Markdown-generated PDF using knitr; you will then need to push this document to the course respository hosted on [IU's GitHub](<https://github.iu.edu>) and generate a pull request.

## SETTING YOUR WORKING DIRECTORY
The first step of working with R is to set your working directory. This is where your R script and output will be saved. It's also a logical place to put data files that you plan to import into R. The following command will return your current working directory:
```{r}
getwd()
```

To change your directory, you can can use the following command (but note that you will need to modify to reflect your actual directory):
```{r}
setwd("~/GitHub/QuantitativeBiodiversity/Assignments/Week1")
```

## USING R AS A CALCULATOR
R is capable of performing various calcuations using simple operators and built-in **functions**

+ _addition_:
```{r}
1 + 3 
```

+ _subtraction_:
```{r}
3 - 1 
```

+ _multiplication_ (with an exponent):
```{r}
3 * 10^2
```

+ _division_ (using a built-in constant):
```{r}
10 / pi 
```

+ _trigonometry_ with a simple built-in **function** (i.e., *sin*) that takes an **argument** (i.e., '4'):
```{r}
sin(4) 
```

+ _logarithms_ (another example of functions and arguments)
```{r}
log10(100) 
log(100)
```

## ASSIGNING VARIABLES
In R, you will often find it useful and necessary to assign values to a variable or **object**. 
Generally speaking, it's best to use `<-` rather than `=` as an assignment operator.
```{r}
a <- 10
b <- a + 20
```

What is the value of b? 

Now let's reassign a new value to `a`:
```{r}
a <- 200
```

Now, what is the value of 'b'? What's going on? 

R holds onto the original value of 'a' that was used when assigning values to 'b'. You can correct this using the `rm` function, which removes objects from your R **environment**. 
```{r}
rm("b")
```

What happens if we reassign `b` now?
```{r}
b <- a + 20
```

Sometimes it's good practice to clear all variables from your R environment (e.g., you've been working on multiple projects). This can be done in a couple of ways. For example, you can just click `clear` in the Environment windwow of R Studio. The same procedure can be performed at the command line. To do this, you can use the `ls` function to view a list of all the objects in the R environment:
```{r}
ls()
```

You can now clear all of the stored variables from R's memory (using two functions: `rm` and `ls`):
```{r}
rm(list=ls())
```

## WORKING WITH VECTORS AND MATRICES

**Vectors** are the fundamental data type in R. Often, vectors are just a collection of data of a similar type, either numeric (e.g., 17.5), integer (e.g., 2), or character (e.g., "low"). The simplest type of vector is a single value, sometimes referred to as a **scalar** in other programming languages:
```{r}
w <- 5 
```

We can create longer one-dimensional vectors in R like this: 
```{r}
x <- c(2, 3, 6, w, w + 7, 12, 14)
```

What is the function `c`() that we just used to create a vector? To answer this question, trying typing `help`() function at the command line. Let's try it out:
```{r}
help(c)
```

What happens when you multiply a vector by a "scalar"?
```{r}
y <- w * x
```

What happens when you multiply two vectors of the same length?
```{r}
z <- x * y 
```

You may need to reference a specific **element** in a vector. We will do this using the square brackets. In this case, the number inside of the square brackets tells R that we want call the second element of vector `z`:
```{r}
z[2]
```

You can also reference multiple elements in a vector:
```{r}
z[2:5] 
```

In some instances, you may want to change the value of an element in a vector. Here's how you can substitute a new value for the second element of `z`:
```{r}
z[2] <- 583 
```

## This should be a new section: How about summary statistics?

It's pretty easy to perform summary statistics on a vector using the built-in fuctions of R:
```{r}
max(z)    # maximum
min(z)    # minimum
sum(z)    # sum
mean(z)   # mean
median(z) # median
var(z)    # variance
sd(z)     # standard deviation
```

What happens when you take the standard error of the mean (`sem`) of z? 

Mario: 
The standard error of the mean is defined as $SEM = \frac{sd (x)}{\sqrt{n}}$. This function does not exist in the base package of R. Therefore, you need to write your own function. Let's give it a try:

Jay:
The standard error of the mean is defined as $sem = \frac{sd (x)}{\sqrt{n}}$. This function does not exist in the base package of R. Sometimes you will need to write your own functions. Let's give it a try:

```{r}
sem <- function(x, ...){
  sd(x)/sqrt(length(na.omit(x)))
  }
```

There are number of functions inside of `sem`. Take a moment to think about and describe what is going on here. 

Often, datasets have missing values (designated as 'NA' in R):
```{r}
i <- c(2, 3, 9, NA, 120, 33, 7, 44.5)
```

What happens when you apply your `sem` function to vector i? This is a problem!

One solution is to tell R to remove NA from the dataset: 
```{r}
sem(i, na.rm = TRUE)
```

** Jay, I think we should just write this as (always removes NA)
```{r}
sem <- function(x){
  sd(x, na.rm = TRUE)/sqrt(length(na.omit(x)))
  }
```

** Maybe my summary statistic idea was bad, sorry.

**Matrices** are just two-dimensional vectors containing data of the same type (e.g., numeric, integer, character).

There are three common ways to create a matrix in R. 

**Approach 1** is to combine (or concatenate) two or more vectors. Let's start by creating a one-dimensional vector using a new function `rnorm`.
```{r}
j <- c(rnorm(length(z), mean = z)) 
```

What does the `rnorm` function do? What are arguments specifying? Remember your friend `help`!

Now we will use the function `cbind` to create a matrix from the two one-dimensional vectors:
```{r}
k <- cbind(z, j) 
```

Use the `help` function to learn about `cbind`.
Use the `dim` function to describe the matrix you just created. What did you learn from this? 

**Approach 2** to making a matrix is to use the `matrix` function along with arguments that specify the number of rows (nrow) and columns (ncol):
```{r}
l <- matrix(c(2, 4, 3, 1, 5, 7), nrow = 3, ncol = 2) 
```

**Approach 3** to making a matrix is to import or *load a dataset* from your working directory:
```{r}
m <- as.matrix(read.table("matrix.txt", sep = "\t", header = FALSE))
```

In this case, we're reading in a tab-delimited file. The name of your file must be in quotes, and you need to specify tab-limited file type using the `sep` argument. The `header` argument tells R whether or not the names of the variables are contained in the first line; in the current example, they are not. 

Often, when handling datasets, we want to be able to transpose a matrix. This is an easy operation in R:
```{r}
n <- t(m)
```

Confirm the transposition using the `dim` function.

Frequently, you will need to **index** or retrieve a certain portion of a matrix. As with the vector example above, we will use the square brackets to retrieve data from a matrix. Inside the square brackets, there are now two subscripts corresponding to the rows and columns, respectively, of the matrix. 

The following code will create a new matrix (`n') based on the first three rows of matrix (`m`):
```{r}
n <- m[1:3, ]
```

Or maybe you want the first two columns of a matrix:
```{r}
n <- m[, 1:2]
```

Or perhaps you want non-sequential columns of a matrix. How do we do that? It's easy when you understand how to reference data within a matrix:
```{r}
n <- m[, c(1:2, 5)]
```

Describe what we just did in the last indexing.

## BASIC DATA VISUALIZATION AND STATISTICAL ANALYSIS

In the following exercise, we will use a dataset from [Lennon et al. (2003)](http://www.indiana.edu/~microbes/publications/Lennon_etal_2003.pdf), which looked at zooplankton community assembly along an experimental nutrient gradient. Inorganic nitrogen and phosphorus were added to mesocosms for six weeks at three different levels (low, medium, and high), but we also direcly measured nutrient concentrations in the mesocosms. So we have categorical and continuous predictors that we're going to use to help explain variaiton in zooplankton biomass.  

The first thing we're going to do is load the data:
```{r}
meso <- read.table("zoop_nuts.txt", sep = "\t", header = TRUE)
```

Let's use the `str` function to look at the structure of the data. 
```{r}
str(meso)
```

How does this dataset differ from the `m` dataset above? We're now dealing with a new type of **data structure**. Specifically, the `meso` dataset is a **data frame** since it has a combination of numeric and character data. 

Here is a description of the column headers:

  + TANK = unique mesocosm identifier

  + NUTS = categorical nutrient treament 
  
  + TP = total phosphorus concentration (µg/L)
  
  + TN = total nitrogen concentration (µg/L)
  
  + SRP = soluble reactive phosphorus concentration (µg/L)
  
  + TIN = total inorganic nutrient concentration (µg/L)
  
  + CHLA = chlorophyll *a* concentration (proxy for algal biomass; µg/L)
  
  + ZP = zooplankton biomass (mg/L)

A common step in data exploration is to look at correlations among variables. Before we do this, let's index our numerical (continuous) data in the 'meso' dataframe. (correlations don't work well on categorical data.)
```{r}
meso.num <- meso[,3:8]
```

We can conveniently visualize pairwise bi-plots of the data using the following command:
```{r}
pairs(meso.num)
```

Now let's conduct a simple (Pearson's) correlation analysis with the R `cor()` function. 
```{r}
cor1 <- cor(meso.num)
```

Describe what you found from the visualization and correlation anaylysis above?

The base pakcage in R won't always meet all of our needs. This is why there are > 6,000 **contributed packages** that have been developed for R. This may seem overwhelming, but it also means that there are tools (and web support) for just about any problem you can think of.  

When using one of the contributed packages, the first thing we need to do is **install** them along with their dependencies (other packages). We're going to start out by using the ['psych' package](http://cran.r-project.org/web/packages/psych/vignettes/overview.pdf), which has many features, but we're going to use it specifically for the `corr.test` function, which generates p-values for each pairwise correlation. (For whatever reason, the `cor` function in the base package does not provide p-values.)

```{r}
require("psych")||install.packages("psych");require("psych") 
```

<-- Should we describe this code?

Now, let's look at the correlations among variables and assess whether they are signficant
```{r}
cor2 <- corr.test(meso.num, method = "pearson", adjust = "BH")
print(cor2, digits = 3)
```

_Notes on corr.test_: 

a) for rank-based correlations (i.e., non-parametric), use method = "kendall" or "spearman". Give it a try!

b) the adjust = "BH" statement supplies the Benjamini & Hochberg-corrected p-values in the upper right diagonal of the square matrix; the uncorrected p-values are below the diagonal. This process corrects for **false discovery rate**, which arises whe making multiple comparisons. 

Let's load another package now that will let us visualize the sign and strength of the correlations:
```{r}
require("corrplot")||install.packages("corrplot");require("corrplot")
corrplot(cor1, method = "ellipse")
```

It seems that TN is a fairly good predictor of ZP and this is something that we directly manipulated. So, let's try some linear regression using the `lm` function:
```{r}
fitreg <- lm(ZP ~ TN, data = meso)
```

Let's examine the output of the regression model:
```{r}
summary(fitreg)
```

Now, let's look at a plot of the data used in our regression:
```{r reglayer1, eval = TRUE} 
TN <- meso$TN # Not needed
ZP <- meso$ZP # Not needed
plot(meso$TN, meso$ZP, ylim = c(0, 10), xlim = c(500, 5000), 
     xlab = expression(paste("Total Nitrogen (", mu,"g/L)")),
     ylab = "Zooplankton Biomass (mg/L)", las = 1)
```

To add the regression line, the first thing we need to do is identify a range of x-values and then generate the corresponding predicted values from our regression model:
```{r reglayer2, echo = -1}
<<reglayer1>>
newTN <- seq(min(TN), max(TN), 10)
regline <- predict(fitreg, newdata = data.frame(TN = newTN))
lines(newTN, regline)
```

Now let's create and plot the 95% confidence intervals using the same procedure as above, that is, use 'newTN' to generate corresponding confidence intervals from our regression model:
```{r reglayer3, echo = -1}
<<reglayer2>>
conf95 <- predict(fitreg, newdata = data.frame(TN = newTN),
                  interval = c("confidence"), level = 0.95, type = "response")
matlines(newTN, conf95[, c("lwr", "upr")], type="l", lty = 2, lwd = 1, col = "black")
```

We should also look at the residuals (i.e., observed values - predicted values) to see if our data meet the assumptions of linear regression. Specifically, we want to make sure that the residuals are normally distributed and that they are homoskedastic (i.e., equal variance). We can look for patterns in our residuals using the following diagnostics:
```{r}
par(mfrow = c(2, 2), mar = c(5.1,4.1,4.1,2.1))
plot(fitreg)
```

We also have the option of looking at the relationship between zooplankton and nutrients where the manipulation is treated categorically (low, medium, high). First, let's order the categorical nutrient treatments from low to high (R's default is to order alphabetically):
```{r}
NUTS <- factor(meso$NUTS, levels = c('low', 'medium', 'high'))
```

Before plotting, we need to calcualte the means and standard errors for zooplankton biomass in our nutrient treatments. We are goint to use an important function called `tapply`. This allows us to apply a function (e.g., mean) to a vector (e.g., ZP) based on information in another column (e.g., nutrient treatment). 
```{r}
sem <- function(x){
  sd(x, na.rm = TRUE)/sqrt(length(na.omit(x)))
  }
zp.means <- tapply(ZP, NUTS, mean)
zp.sem <- tapply(ZP, NUTS, sem)
```

Now let's make the barbplot:
```{r boxlayer1, eval = TRUE}
bp <- barplot(zp.means, ylim =c(0, round(max(ZP), digits = 0)), 
              pch = 15, cex = 1.25, las = 1, cex.lab = 1.4, cex.axis = 1.25, 
              xlab = "nutrient supply", 
              ylab = "zooplankton biomass (mg/L)",
              names.arg = c("low", "medium", "high"))
bp ## not needed?
```

We need to add the error bars (+/- sem) "manually" as follows:
```{r boxlayer2, echo = -1}
<<boxlayer1>>
arrows(x0 = bp, y0 = zp.means, y1 = zp.means - zp.sem, angle = 90,
       length=0.1, lwd = 1)
arrows(x0 = bp, y0 = zp.means, y1 = zp.means + zp.sem, angle = 90,
       length=0.1, lwd = 1)
```

We can conduct a one-way analysis of variance (ANOVA) as follows:
```{r}
fitanova <- aov(ZP ~ NUTS, data = meso)
```

Let's look at the output in more detail (just as we did with regression):
```{r}
summary(fitanova)
```

Finally, we can conduct a post-hoc comparison of treatments using Tukey's HSD (Honest Significant Differences). This will tell us whether or not their are differences (alpha = 0.05) among pairs of the three nutrient treatments
```{r}
TukeyHSD(fitanova)
```

Just like the regression analysis above, it's good to look at the residuals:
```{r}
par(mfrow = c(2, 2), mar = c(5.1,4.1,4.1,2.1))
plot(fitanova)
```


<-- Similar to SAS output
```{r}
require("agricolae")||install.packages("agricolae");require("agricolae")
HSD.test(fitanova, "meso$NUTS", group = TRUE, console = TRUE)
```

##HOMEWORK
1) Recreate this exercise
2) Redo the regression and ANOVA with log10-transformed zooplankton biomas data (often, log-transformations help with meeting assumptions of normality and equal variance)
3) use knitr to create a pdf, push it to GitHub, and create a pull request


