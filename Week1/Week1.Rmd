---
title: "Week 1 Exercise: Basic R"
author: "Z620: Quantitative Biodiversity, Indiana University"
date: "January 16, 2015"
output: pdf_document
---
In this exercise, we provide an introduction to some of the basic features of the R computing environment. We will briefly cover operators, data types, and simple commands that will be useful for you during the course and beyond. 

You are working within an [R Markdown](<http://rmarkdown.rstudio.com>)document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. When you click the **Knit** button in of the scripting window a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. If there are errors in your markdown document, you will not be able to knit a PDF file. Assignments in this class will require that you create a PDF via knitr and submit it via [GitHub](<http://github.com>).

## SETTING YOUR WORKING DIRECTORY
The first step of working with R is to set your working directory. This is where your R script and output will be saved. It's also a logical place to put data files that you plan to import into R. 

The following command will return your current working directory
```{r}
getwd()
```

If you want to change your working directory, use the following command (you will need to change this to reflect your desired directory):
```{r}
setwd("~/GitHub/Quantitative_Biodiversity/Assignments/Week1")
```

## USING R AS A CALCULATOR
R is capable of performing various calcuations using simple operators and built-in **functions**

addition
```{r}
1 + 3 
```

subtraction
```{r}
3 - 1 
```

multiplication (with an exponent)
```{r}
3 * 10^2
```

division (using a built-in constant)
```{r}
10 / pi 
```

trigonometry with a simple built-in **function** (i.e., *sin*) that takes an **argument** (i.e., '4')
```{r}
sin(4) 
```

logarithms (another example of functions and arguments)
```{r}
log10(100) 
log(100)
```

## ASSIGNING VARIABLES
In R, you will often find it useful and necessary to assign values to a variable or **object**. 
Generally speaking, it's best to use `<-` rather than `=` as an assignment operator.
```{r}
a <- 10
b <- a + 20
```

What is the value of b? 

Now let's reassign a new value to `a`
```{r}
a <- 200
```

Now, what is the value of 'b'? What's going on? 

R holds onto the original value of 'a' that was used when assigning values to 'b'. You can correct this using the `rm` function, which removes objects from your R **environment**. 
```{r}
rm("b")
```

What happens if we reassign `b` now?
```{r}
b <- a + 20
```

In some situations it's a good idea to clear all variables from your R environment. This can be done in a couple of ways. For example, you can just click `clear'in the Environment windwow of R Studio. The same procedure can performed at the command line. The *ls* function allows us to view a list of objects in our R environment:
```{r}
ls()
```

We can now clear all of the stored variables from R's memory (using two functions: `rm` and 'ls')
```{r}
rm(list=ls())
```

## WORKING WITH VECTORS AND MATRICES

**Vectors** are the fundamental data type in R. Often, vectors are just a collection of data of a similar type, either numeric, integer, or character. The simplest type of vector is a single value, sometimes referred to as a **scalar** in other programming languages:
```{r}
w <- 5 
```

We can create longer one-dimensional vectors in R like this: 
```{r}
x <- c(2, 3, 6, w, w + 7, 12, 14)
```

What is the function `c()`? The `help()` function is your friend. Let's try it out:
```{r}
help(c)
```

What happens when you multiply a vector by a "scalar"?
```{r}
y <- w * x
```

What happens when you multiply two vectors of the same length?
```{r}
z <- x * y 
```

You may want to reference a specific element in a vector. We will do this using the square brackets. In this case, the number inside of the square brackets tells R that we want the second element of vector `z`:
```{r}
z[2]
```

You may also need to reference multiple elements in a vector:
```{r}
z[2:5] 
```

In some instances, you may want to change the value of an element in a vector. Here's how you can substitute a new value for the second element of `z`:
```{r}
z[2] <- 583 
```

It's pretty easy to perform summary statistics on a vector using the built-in fuctions of R:
```{r}
max(z)    # maximum
min(z)    # minimum
sum(z)    # sum
mean(z)   # mean
median(z) # median
var(z)    # variance
sd(z)     # standard deviation
```

What happens when you take the standard error of the mean (`sem`) of z? 
The standard error of the mean is defined as $SEM = \frac{sd (x)}{\sqrt{n}}$. This function does not exist in the base package of R. Therefore, you need to write your own functions sometimes. Let's give it a try:
```{r}
sem <- function(x, ...){
  sd(x, ...)/sqrt(length(na.omit(x)))
  }
```
There are number of functions inside of `sem`. Take a moment to think about and describe what is going on here. 

Often, datasets have missing values (designated as 'NA' in R)
```{r}
i <- c(2, 3, 9, NA, 120, 33, 7, 44.5)
```

What happens when you apply your `sem` function to vector i? 

One solution is to tell R to remove NA from the dataset: 
```{r}
sem(i, na.rm = TRUE)
```

**Matrices** are just two-dimensional vector, with the stipulation that we discusse above that all of the data are of the same type (e.g., numeric, integer, character).

There are three common ways to create a matrix (two dimensional vectors) in R. 

**Approach 1** is to combine (or concatenate) two or more vectors. Let's start by creating a one-dimensional vector using a new function `rnorm` 

<-- seems odd to introduce this here. Also, why not NAs? -- MEM
<-- Doesn't seem odd to me, but I'm defintiely open to discuss -- JTL

```{r}
j <- c(rnorm(length(z), mean = z)) 
```

What does the `rnorm` function do? What are arguments doing? Remember your friend `help`!

Now we will use the function `cbind` to create a matrix from the two one-dimensional vectors:
```{r}
k <- cbind(z, j) 
```

Use the `help` function to learn about `cbind`
Use the `dim` function to describe the matrix you just created 

**Approach 2** to making a matrix is to use the `matrix` function along with arguments that specify the number of rows (nrow) and columns (ncol):
```{r}
l <- matrix(c(2, 4, 3, 1, 5, 7), nrow = 3, ncol = 2) 
```

**Approach 3** to making a matrix is to import or *load a dataset* from  your working directory:
```{r}
m <- as.matrix(read.table("matrix.txt", sep = "\t", header = FALSE))
```
In this case, we're reading in a tab-delimited file. The name of your file must be in quotes, and you specify tab-limited file type using the `sep` argument. The `header` argument tells R whether or not the names of the variables are contained in the first line; in the current example, that is not the case. 

Often, when handling datasets, we want to be able to transpose a matrix. This is an easy operation in R:
```{r}
n <- t(m)
```
Confirm the transposition using the `dim` function

Frequently, you will need to **index** or retrieve a certain portion of a matrix. As with the vector example above, we will use the square brackets to retrieve data from a matrix. Inside the square brackets, however, there are now two subscripts corresponding to the rows and columns, respectively, of the matrix. 

For example, the following code will create a new matrix (`n') based on the first three rows of matrix (`m`):
```{r}
n <- m[1:3, ]
```

Or maybe you want the first two columns of a matrix:
```{r}
n <- m[, 1:2]
```

Or perhaps you want non-sequential columns of a matrix. How do we do that? 

It's easy when you understand how to reference data within a matrix:
```{r}
n <- m[, c(1:2, 5)]
```

Describe what we just did in the last indexing.


## Basic Data Visualization and Statistical Analysis

In the following exercise, we will use a dataset from [Lennon et al. (2003)](http://www.indiana.edu/~microbes/publications/Lennon_etal_2003.pdf), which looked at zooplankton community assembly along an experimental nutrient gradient. Inorganic nitrogen and phosphorus were added to mesocosms for six weeks at three different levels (low, medium, and high), but we also direcly measured nutrient concentrations. So we have categorical and continuous predictors that we're going to use to help explain variaiton in zooplankton biomass.  

The first thing we're going to do is load the data:
```{r}
meso <- read.table("zoop_nuts.txt", sep = "\t", header = TRUE)
```

Let's use the `str` function to look at the structure of the data. 
```{r}
str(meso)
```

How does this dataset differ from the 'm' dataset above? We're now dealing with a new type of **data structure**. Specifically, the `meso` dataset is a **data frame** since it has a combination of numeric and character data. 

Let's explore the data set a little bit:
  -TANK = mesocosm identifier
  
  -NUTS = categorical nutrient treament 
  
  -TP = total phosphorus concentration (µg/L)
  
  -TN = total nitrogen concentration (µg/L)
  
  -SRP = soluble reactive phosphorus concentration (µg/L)
  
  -TIN = total inorganic nutrient concentration (µg/L)
  
  -CHLA = chlorophyll *a* (proxy for algal biomass; µg/L)
  
  -ZP = zooplankton biomass (mg/L)

Let's look for any correlations among these variables. But before we do this, let's index our numerical (continous) data in the 'meso' dataframe. (This will make for a little less typing.)
```{r}
meso.num <- meso[,3:8]
```

Now, let's use simple correlation analysis with the R `cor()` function. 
```{r}
cor1 <- cor(meso.num)
```

The base pakcage in R won't always meet all of our needs, however. This is why there are > 6,000 additional packages that have been developed for R. Although this may seem overwhelming, it also means that there are a lot of really cool tools that people have created for almost any problem you can think of. Here are a few examples of packages that introduce some more bells and whistles for performing correlation analyses. 

The first thing we need to do is install the non-base packages and their dependencies. 
We're going to start out by using the ['psych' package](http://cran.r-project.org/web/packages/psych/vignettes/overview.pdf), which has many features, but we're going to use it specifically for the `corr.test` function, which generates p-values for each pairwise correlation. (For whatever reason, the `cor` function in the base package does not provide p-values.)

--> Stopped here

```{r}
#require("psych")||install.packages("psych");require("psych")
#install.packages("psych")
require(psych)
```

Now, let's look at the correlations among variables and assess whether they are signficant:
```{r}
cor2 <- corr.test(meso.num, use = "pairwise", method = "pearson", 
                  adjust = "hochberg")
print(cor2, digits = 3)
```

Let's load another package now that will let us visualize the sign and stregth of the correlations:
```{r}
#require("corrplot")||install.packages("corrplot");require("corrplot")
require(corrplot)
corrplot(cor1, method = "ellipse")
```
It seems that TN is a fairly good predictor of ZP and this is something that we directly manipulated. So, let's try some linear regression and look at some ofthe summary statistics
```{r}
fit <- lm(ZP ~ TN, data = meso)
summary(fit)
```
It's good practice to look at the residuals of your regression model to make sure you're meeting major assumption of the test. We can look for patterns in the residuals as follows:
```{r}
op <- par(no.readonly = TRUE) # the whole list of settable par's.
## do lots of plotting and par(.) calls, then reset:
par(op)
par(mfrow=c(2,2))
plot(fit)
par(op)
```
Let's start off by making a scatter plot
```{r}
TN <- meso$TN
ZP <- meso$ZP
plot(TN,ZP,ylim=c(0,10),xlim=c(500,5000),xlab=expression(paste("Total Nitrogen (", mu,"g/L)")),ylab="Zooplankton Biomass (mg/L)",las=1)
```
Now lets plot the regression line. The first thing we need to do is generate a range of x-values and then get the corresponding predicted values
```{r}
newTN <- seq(min(TN),max(TN),10)
regline <- predict(fit,newdata=data.frame(TN=newTN))
lines(newTN,regline)
```
Now lets generate and plot the 95% confidence intervals
```{r}
conf95 <- predict(fit,newdata=data.frame(TN=newTN),interval=c("confidence"),level=0.95,type="response")
matlines(newTN,conf95[,c("lwr","upr")],type="l",lty=2,lwd=1,col="black")
```
We also have the option of looking at the relationship between nutrients and zooplankton in a categorical perspective. Let's order the categorical nutrient treatments:
```{r}
NUTS <- factor(meso$NUTS,levels = c('low','medium','high'))
```
Now let's make a barplot. The first thing we need to do is calcualte the means and standard errors for zooplankton biomass in our nutrient treatments 
```{r}
sem <- function(x, ...){
  sd(x, ...)/sqrt(length(na.omit(x)))
}
zp.means <- tapply(ZP, NUTS, mean)
zp.sem <- tapply(ZP, NUTS, sem)
```
Now let's make the barbplot
```{r}
bp <-barplot(zp.means, ylim=c(0,round(max(ZP), digits=0)), pch=15, cex=1.25, las=1, xlab="nutrient supply", ylab="zooplankton biomass (mg/L)",
             cex.lab=1.4, cex.axis=1.25, names.arg=c("low","medium","high"))
```
Now let's add the error bars
```{r}
arrows(x0 = bp, y0 = zp.means, y1 = zp.means - zp.sem, angle = 90,
       length=0.1, lwd = 1) # 
arrows(x0 = bp, y0 = zp.means, y1 = zp.means + zp.sem, angle = 90,
       length=0.1, lwd = 1)
```



par(mfrow=c(1,1), mar=c(4,5,4,4), oma=c(0,3,0,0))





Included in R and various R packages are some basic datasets that are useful for testing functions and learning about R features and functions. One such dataset is **cars**. To learn about this dataset you can simple use the `{r} help` function

```{r}
help(cars)
```
Use the `{r} str()` and `{r} summary()' functiosn to see basic summary statistics about this dataset
```{r}
str(cars)
summary(cars)
```
To visualize this data you can generate a simple plot with the `{r} plot()` function
```{r}
plot(cars)
```
You can also embed plots, for example: # JTL, line by line, got an error; plus not sure how useful? what's point?
```{r, echo=FALSE}
plot(cars)
```
Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.'

## Other Useful Features and Fucntions: Sorting, Subsetting, Sampling
##JTL: seems like some of this could be combined with stuff above
**Sorting**
We can use another dataset (mtcars) to practice sorting (ordering) data. Learn about mtcars via `{r} help(mtcars)`

sort by mpg
```{r}
newdata <- mtcars[order(mtcars$mpg),]
```
sort by mpg and cyl # JTL: not sure how effect the cyl sort is
```{r}
newdata <- mtcars[order(mtcars$mpg, mtcars$cyl),]
```
sort by mpg (ascending) and cyl (descending) #JTL: same as above?
```{r}
newdata <- mtcars[order(mtcars$mpg, - mtcars$cyl),]
```
Now, Let's make a new vector of data
```{r}
z <- c(1.5, 1/6, 1/3)
```
If we only want to view  the first two decimal places of z
```{r}
round(z,2)
```
Now, we can reverse the order of the elements in z
```{r}
rev(z)
```
And we can order z from smallest to largest
```{r}
sort(z)
```
We can also identify the ordering of z #JTL: with respect to what?
```{r}
order(z)
```
i.e., the 2nd number is the min and the 1st number is the max

Additionally, we can idenify the maximum values this way:
```{r}
max(z)
```
**Subsetting**
Let's create a original object vector, x:
```{r}
x <- c(3, 4, 7)
x
```
Now, let's subset this vector and keep only the first three values
```{r}
x[-3]
```
Now, let's subset this vector and keep only the velues greater than or equal to 5
```{r}
x[x >= 5]
```
Notice that we did this using a logic statement `{r} >=`. Here is a list of other logica operators that you might find useful:

|Logic Operator|Meaning| # confusing to start using new symbols "|"?
|! x | Is Not "x"|
|x & y| "x" and "y" (element by element) |
|x && y| "x" and "y" (across all elements)|
|x `|` y | "x" or "y" (element by element)|
|x `||` y | "x" or "y" (across all elments)|

You can learn more about this commands (`{r} help(Logic, package=base))


**Sampling**








First, let's create a sequence of numbers
```{r}
seq(1,3,length=5)

# Create the same sequence in a slightly different way:
seq(1,3,by=0.5)

# Create another sequence by going from 3 to 1:
seq(3,1,by= -0.5)
```

To randomly sample from an existing vector:
```{r}
sample(x,10,replace=T)
```

Or to randomly sample from a sequence of numbers from 1 to 500:
```{r}
sample(1:500,10,replace=F)
```



