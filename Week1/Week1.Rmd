---
title: "Week 1 Exercise: Basic R"
author: "Z620: Quantitative Biodiversity, Indiana University"
date: "January 16, 2015"
output: pdf_document
---
In this exercise, we provide an introduction to some of the basic features of the R computing environment. We emphasize calcuations, data types, and simple commands that will be useful for you during the course and beyond. 

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using [R Markdown](<http://rmarkdown.rstudio.com>).When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. 

## SETTING YOUR WORKING DIRECTORY (explain what and why)
```{r}
getwd()
#The following line needs to be updated
#setwd("~/GitHub/Quantitative_Biodiversity/Assignments/Week1")
```
## USING R AS A CALCULATOR
addition
```{r}
1 + 3 
```
subtraction
```{r}
3 - 1 
```
multiplication (with exponent)
```{r}
3 * 10^2
```
division (using a built-in constant)
```{r}
10 / pi 
```
trigonometry with a simple built-in **function** (i.e., *sin*) and **argument** (i.e., '4')
```{r}
sin(4) 
```
logarithms (another example of function and argument)
```{r}
log10(100) 
log(100)
```
## DEFINING VARIABLES
In R, you will often find it useful and necessary to assign values to a variable. Generally speaking, it's best to use `<-` rather than `=` as an assignment operator.
```{r}
a <- 10
b <- a + 20
```
What is the value of b?
```{r}
a <- 200
```
Now what is the value of b? Can you explain? Fix? It can help to examine variables with the following function 
```{r}
ls()
```
You can clear variables from R memory with following function (example of nested function)
```{r}
rm(list=ls())
```
You can also examine variables in the Environment windwow of R Studio. By clikcing 'clear' in this window, you can erase variables from memory

### --> Time for discussing R as a caculator and assigning of variables


## WORKING WITH SCALARS, VECTORS, AND MATRICES
There is a hierarchy of mathematical elements. First, a **scalar** is single numeric value. Let's assign a numeric value to a character:
```{r}
w <- 5 
```
A **vector** (or array) is a one-dimensional row of numeric values. You can create a vector in R:
like this:
```{r}
x <- c(2, 3, 6, w, w + 7, 12, 14)
```
What is the function `c()`? The `help()` function is your friend. Let's try it out:
```{r}
help(c)
```
What happens when you multiply a vector by a scalar?
```{r}
y <- w * x
```
What happens when you multiply two vectors?
```{r}
z <- x * y 
```
Here is how you reference an element in a vector
```{r}
z[2]
```
Here is how you reference multiple elements in a vector
```{r}
z[2:5] 
```
Here is how you can change the value of an element in a vector
```{r}
z[2] <- 583 
```
It's pretty easy to perform summary statistics on a vector using built-in fuctions
```{r}
max(z)    # maximum
min(z)    # minimum
sum(z)    # sum
mean(z)   # mean
median(z) # median
var(z)    # variance
sd(z)     # standard deviation
```
What happens when you take the standard error of the mean (`sem`) of z? The standard error of the mean is defined as $SEM = \frac{sd (x)}{\sqrt{n}}$ Sometimes you need to make your own functions. Let's give it a try:
```{r}
sem <- function(x, ...){
  sd(x, ...)/sqrt(length(na.omit(x)))
  }
```
Often, datasets have missing values (designated as 'NA' in R)
```{r}
i <- c(2, 3, 9, NA, 120, 33, 7, 44.5)
```
What happens when you apply your `sem` function to vector i? One solution is to tell R to remove NA from the dataset:
```{r}
sem(i, na.rm = TRUE)
```
There are three common ways to create a matrix (two dimensional vectors) in R. **Approach 1** is to combine (or concatenate) two or more vectors. Let's start by creating a vector using a new function `rnorm`
```{r}
j <- c(rnorm(length(z), mean = z)) 
```
What does the `rnorm` function do? What are arguments doing?
Now we will use the function `cbind` to create a matrix
```{r}
k <- cbind(z, j) 
```
Use the `help` function to learn about `cbind`
Use the `dim` function to describe the matrix you just created 

**Approach 2** to making a matrix is to use the matrix function:
```{r}
l <- matrix(c(2, 4, 3, 1, 5, 7), nrow = 3, ncol = 2) 
```
**Approach 3** to making a matrix is to import or *load a dataset* from  your working directory (or elsewhere)
```{r}
m <- as.matrix(read.table("matrix.txt", sep = "\t", header = FALSE))
```
Often, when handling datasets, we want to be able to transpose a matrix. This is easy in R:
```{r}
n <- t(m)
```
Also, you will find that you need to subset data in a matrix. For example, maybe you want to take first three rows of a matrix:
```{r}
n <- m[1:3, ]
```
Or maybe you want the first two columns of a matrix:
```{r}
n <- m[, 1:2]
```
Or perhaps you want non-sequential columns of a matrix. How do we do that? It's easy when you understand how to reference data within a matrix:
```{r}
n <- m[, c(1:2, 5)]
```
## Basic Data Visualization and Statistical Analysis
In the following exercise, we will use a dataset from [Lennon et al. (2003)](http://www.indiana.edu/~microbes/publications/Lennon_etal_2003.pdf), which looked at zooplankton community assembly along a trophic state gradient. Nutrients (nitrogen and phosphorus) were added to mesocosms over the course of six weeks at three different levels (low, medium, and high). We also direcly measured nutrients. So we have categorical and continuous predictors that we're going to use to help explain variaiton in zooplankton biomass.  

The first thing we're going to do is load the data:
```{r}
meso <- read.table("zoop_nuts.txt", sep = "\t", header = TRUE)
```
Let's use the `str` function to look at the structure of the data. How does this dataset differ from the 'm' dataset above?
```{r}
str(meso)
```
Now, let's explore the data set a little bit:
  -TANK = mesocosm identifier
  
  -NUTS = categorical nutrient treament 
  
  -TP = total phosphorus concentration ($\mu$g/L)
  
  -TN = total nitrogent concentration
  
  -SRP = soluble reactive phosphorus concentration
  
  -TIN = total inorganic nutrient concentration
  
  -CHLA = chlorophyll *a* (proxy for algal biomass)
  
  -ZP = zooplankton biomass 

Let's look for any correlations among these variables. First, let's assign continuous numerical data in the 'meso' dataframe to make for less typing
```{r}
meso.num <- meso[,3:8]
```
Now, let's use simple correlation analysis with the base package of R with the `cor` function. 
```{r}
cor1 <- cor(meso.num)
```
The base pakcage won't always meet all of our needs, however. This is why there are more than 6,000 additional packages that have been developed for R. Although this can be overwhelming, it also means that there are a lot of really cool tools that people have created for various applications and problems. Here are a few examples of packages that introduce some more bells and whistles for doing correlations. 

The first thing we need to do is install the non-base packages and their dependencies. We're going to start out by using the ['psych' package](http://cran.r-project.org/web/packages/psych/vignettes/overview.pdf), which has many features, but we're going to use it specifically for the `corr.test` function, which generates p-values for each pairwise correlation.
```{r}
#install.packages("psych")
#require(psych)
```
Now, let's look at the correlations among variables and assess whether they are signficant:
```{r}
cor2 <- corr.test(meso.num,use="pairwise",method="pearson",adjust="hochberg")
print(cor2,digits=3)
```
Let's load another package now that will let us visualize the sign and stregth of the correlations:
```{r}
#install.packages("corrplot")
#require(corrplot)
corrplot(cor1, method = "ellipse")
```
It seems that TN is a fairly good predictor of ZP and this is something that we directly manipulated. So, let's try some linear regression and look at some ofthe summary statistics
```{r}
fit <- lm(ZP ~ TN, data = meso)
summary(fit)
```
It's good practice to look at the residuals of your regression model to make sure you're meeting major assumption of the test. We can look for patterns in the residuals as follows:
```{r}
layout(matrix(c(1,2,3,4),2,2))
plot(fit)
```
Let's start off by making a scatter plot
```{r}
TN <- meso$TN
ZP <- meso$ZP
plot(TN,ZP,ylim=c(0,10),xlim=c(500,5000),xlab=expression(paste("Total Nitrogen (", mu,"g/L)")),ylab="Zooplankton Biomass (mg/L)",las=1)
```
Now lets plot the regression line. The first thing we need to do is generate a range of x-values and then get the corresponding predicted values
```{r}
newTN <- seq(min(TN),max(TN),10)
regline <- predict(fit,newdata=data.frame(TN=newTN))
lines(newTN,regline)
```
Now lets generate and plot the 95% confidence intervals
```{r}
conf95 <- predict(fit,newdata=data.frame(TN=newTN),interval=c("confidence"),level=0.95,type="response")
matlines(newTN,conf95[,c("lwr","upr")],type="l",lty=2,lwd=1,col="black")
```
We also have the option of looking at the relationship between nutrients and zooplankton in a categorical perspective. Let's order the categorical nutrient treatments:
```{r}
NUTS <- factor(meso$NUTS,levels = c('low','medium','high'))
```
Now let's make a barplot. The first thing we need to do is calcualte the means and standard errors for zooplankton biomass in our nutrient treatments 
```{r}
sem <- function(x, ...){
  sd(x, ...)/sqrt(length(na.omit(x)))
}
zp.means <- tapply(ZP, NUTS, mean)
zp.sem <- tapply(ZP, NUTS, sem)
```
Now let's make the barbplot
```{r}
bp <-barplot(zp.means, ylim=c(0,round(max(ZP), digits=0)), pch=15, cex=1.25, las=1, xlab="nutrient supply", ylab="zooplankton biomass (mg/L)",
             cex.lab=1.4, cex.axis=1.25, names.arg=c("low","medium","high"))
```
Now let's add the error bars
```{r}
arrows(x0 = bp, y0 = zp.means, y1 = zp.means - zp.sem, angle = 90,
       length=0.05, lwd = 2) # 
arrows(x0 = bp, y0 = zp.means, y1 = zp.means + zp.sem, angle = 90,
       length=0.05, lwd = 2)
```



par(mfrow=c(1,1), mar=c(4,5,4,4), oma=c(0,3,0,0))





Included in R and various R packages are some basic datasets that are useful for testing functions and learning about R features and functions. One such dataset is **cars**. To learn about this dataset you can simple use the `{r} help` function

```{r}
help(cars)
```
Use the `{r} str()` and `{r} summary()' functiosn to see basic summary statistics about this dataset
```{r}
str(cars)
summary(cars)
```
To visualize this data you can generate a simple plot with the `{r} plot()` function
```{r}
plot(cars)
```
You can also embed plots, for example: # JTL, line by line, got an error; plus not sure how useful? what's point?
```{r, echo=FALSE}
plot(cars)
```
Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.'

## Other Useful Features and Fucntions: Sorting, Subsetting, Sampling
##JTL: seems like some of this could be combined with stuff above
**Sorting**
We can use another dataset (mtcars) to practice sorting (ordering) data. Learn about mtcars via `{r} help(mtcars)`

sort by mpg
```{r}
newdata <- mtcars[order(mtcars$mpg),]
```
sort by mpg and cyl # JTL: not sure how effect the cyl sort is
```{r}
newdata <- mtcars[order(mtcars$mpg, mtcars$cyl),]
```
sort by mpg (ascending) and cyl (descending) #JTL: same as above?
```{r}
newdata <- mtcars[order(mtcars$mpg, - mtcars$cyl),]
```
Now, Let's make a new vector of data
```{r}
z <- c(1.5, 1/6, 1/3)
```
If we only want to view  the first two decimal places of z
```{r}
round(z,2)
```
Now, we can reverse the order of the elements in z
```{r}
rev(z)
```
And we can order z from smallest to largest
```{r}
sort(z)
```
We can also identify the ordering of z #JTL: with respect to what?
```{r}
order(z)
```
i.e., the 2nd number is the min and the 1st number is the max

Additionally, we can idenify the maximum values this way:
```{r}
max(z)
```
**Subsetting**
Let's create a original object vector, x:
```{r}
x <- c(3, 4, 7)
x
```
Now, let's subset this vector and keep only the first three values
```{r}
x[-3]
```
Now, let's subset this vector and keep only the velues greater than or equal to 5
```{r}
x[x >= 5]
```
Notice that we did this using a logic statement `{r} >=`. Here is a list of other logica operators that you might find useful:

|Logic Operator|Meaning| # confusing to start using new symbols "|"?
|! x | Is Not "x"|
|x & y| "x" and "y" (element by element) |
|x && y| "x" and "y" (across all elements)|
|x `|` y | "x" or "y" (element by element)|
|x `||` y | "x" or "y" (across all elments)|

You can learn more about this commands (`{r} help(Logic, package=base))


**Sampling**








First, let's create a sequence of numbers
```{r}
seq(1,3,length=5)

# Create the same sequence in a slightly different way:
seq(1,3,by=0.5)

# Create another sequence by going from 3 to 1:
seq(3,1,by= -0.5)
```

To randomly sample from an existing vector:
```{r}
sample(x,10,replace=T)
```

Or to randomly sample from a sequence of numbers from 1 to 500:
```{r}
sample(1:500,10,replace=F)
```



